# Task 5 - Decision Trees and Random Forests

## Objective

Learn tree-based machine learning models for classification using Decision Trees and Random Forests. Evaluate and interpret model performance, feature importance, and overfitting.

---

##  Dataset Used

**Heart Disease Dataset**  
Source: [Kaggle](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)  
This dataset contains 14 features and a target column indicating presence of heart disease.

---

##  Tools and Libraries

- Python
- Pandas, NumPy
- scikit-learn
- Matplotlib, Seaborn
- Graphviz (for decision tree visualization)

---

##  Tasks Performed

1. **Data Loading and Preprocessing**
   - Loaded CSV dataset and split into features (X) and target (y).
   - Performed train-test split.

2. **Decision Tree Classifier**
   - Trained a Decision Tree on the training data.
   - Visualized the tree structure.
   - Analyzed overfitting by plotting accuracy vs. depth.

3. **Random Forest Classifier**
   - Trained a Random Forest with 100 trees.
   - Compared accuracy to Decision Tree.
   - Evaluated using 5-fold cross-validation.

4. **Feature Importance**
   - Extracted and visualized feature importances from the Random Forest.

---

##  Results

- Decision Tree Accuracy: ~[your value]  
- Random Forest Accuracy: ~[your value]  
- Cross-validation Accuracy: ~[your value]  
- Most important features: [e.g., 'cp', 'thalach', 'exang']

---

##  Files Included

- `task5_decision_tree_random_forest.ipynb`: Jupyter Notebook with all code and plots.
- `README.md`: This file.

---

##  Conclusion

This task helped explore:
- How decision trees split data based on features.
- How to control overfitting using depth.
- The power of ensemble learning using Random Forests.
- Interpreting feature importance to gain insights.

---

##  Submission

GitHub Repo Link: [Paste your repository link here]  
Submit via: [https://forms.gle/8Gm83s53KbyXs3Ne9](https://forms.gle/8Gm83s53KbyXs3Ne9)
